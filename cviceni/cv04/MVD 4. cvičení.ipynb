{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 4. cvičení\n",
    "\n",
    "## 1. část - Načtení dat\n",
    "\n",
    "Po rozbalení archive.zip uvidíte articles csv soubor. Tento soubor pochází z [Kaggle datasetů](https://www.kaggle.com/hsankesara/medium-articles) a obsahuje malé množství Medium článků k tématům ML, AI a data science. K úloze dnešního cvičení bude stačit využítí dat s názvy a obsahy článků (title a text).\n",
    "\n",
    "\n",
    "### Příprava dat\n",
    "\n",
    "Pro přípravu dat se použivá různá sekvence kroků. Je doporučeno na následující kroky vytvořit samostatnou funkci, aby bylo možné zpracovat i vyhledávaný výraz při testování. Dnešní cvičení by mělo obsahovat následující kroky:\n",
    "\n",
    "1. Převést všechen text na lower case\n",
    "2. Odstranění interpunkce a všech speciálních znaků (apostrof, ...)\n",
    "3. Aplikace lemmatizátoru\n",
    "\n",
    "Pozn.: Jedná se pouze o jednoduchý preprocessing, v praxi je často potřeba použití více kroků. Tato aplikace by měla například problém s čísly (desetinná čísla, čísla vyhledávaná slovně). \n",
    "\n",
    "Pro lemmatizaci použijte knihovnu spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/filip/.local/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (806 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.5/806.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/filip/.local/lib/python3.10/site-packages (from spacy) (1.23.3)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /home/filip/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/filip/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, click, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.3 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 tqdm-4.64.1 typer-0.4.2 wasabi-0.10.1\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/filip/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/filip/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.3)\n",
      "Requirement already satisfied: jinja2 in /home/filip/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/filip/.local/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Instalace spaCy z Jupyter Notebooku\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Stažení modelu pro angličtinu\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/miniconda3/envs/mvd/lib/python3.10/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # NLTK\n",
    "# Lemmatizace textu př.:  \n",
    "# \" \".join([token.lemma_ for token in lemmatizer(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> tuple:\n",
    "    with open(file_path, 'rt') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        data = [article for article in reader]\n",
    "        return (data[0], data[1:])\n",
    "    \n",
    "def convert_to_lower(fields_id: list, articles: list) -> None:\n",
    "    for article in articles:\n",
    "        for field_id in fields_id:\n",
    "            article[field_id] = article[field_id].lower()\n",
    "            \n",
    "def remove_chars(fields_id: list, articles: list, to_be_removed: re.Pattern) -> None:\n",
    "    for article in articles:\n",
    "        for field_id in fields_id:\n",
    "            article[field_id] = re.sub(to_be_removed, '', article[field_id])\n",
    "            article[field_id] = re.sub('\\s', ' ', article[field_id])\n",
    "\n",
    "def lemmatize(fields_id: list, articles: list, lemmatizer: spacy.lang.en.English) -> None:\n",
    "    for article in articles:\n",
    "        for field_id in fields_id:\n",
    "            article[field_id] = \" \".join([token.lemma_ for token in lemmatizer(article[field_id])])\n",
    "            \n",
    "header, data = load_data('archive/articles.csv')\n",
    "\n",
    "convert_to_lower([4, 5], data)\n",
    "to_be_removed = r'[.,?!/\\\\\\\"`\\-:()\\[\\]*|—’–]'\n",
    "remove_chars([4, 5], data, to_be_removed)\n",
    "lemmatize([4, 5], data, lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - Vytvoření invertovaného indexu\n",
    "\n",
    "Před další prací s textem je potřeba vytvořit invertovaný index, který poté usnadní práci. Invertovaný index bude slovník, kde klíčem bude slovo a hodnotou bude list s id dokumentů (index), které dané slovo obsahují.\n",
    "\n",
    "Pozn.: Je potřeba vytvořit dva invertované indexy - jeden pro title a druhý pro text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(field_id: int, articles: list) -> dict:\n",
    "    index = {}\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        words = [word.strip() for word in re.split(r'\\s', article[field_id]) if word != '']\n",
    "        \n",
    "        for word in words:\n",
    "            if word in index:\n",
    "                index[word].append(i)\n",
    "            else:\n",
    "                index[word] = [i]\n",
    "    \n",
    "    return index\n",
    "\n",
    "title_index = create_index(4, data)\n",
    "content_index = create_index(5, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. část - Implementace TF-IDF\n",
    "\n",
    "Připravení funkce pro výpočet TF-IDF po příchodu dotazu. Funkce *tf_idf* by měla pracovat s dotazem, jedním invertovaným indexem a s danými dokumenty. Vrátit by měla list obsahující skóre pro každý dokument.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\n",
    "score(q,d) = TF\\_IDF(q,d) = \\sum\\limits_{w \\in q \\cap d} c(w, q) c(w, d) log(\\frac{M+1}{df(w)})\n",
    "$\n",
    "</center>\n",
    "\n",
    "$q$ ... dotaz<br>\n",
    "$d$ ... dokument<br>\n",
    "$c(w, q)$ ... kolikrát je slovo *w* v dotazu *q*<br>\n",
    "$M$ ... celkový počet dokumentů<br>\n",
    "$df(w)$ ... počet dokumentů, ve kterých se nachází slovo *w*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query_string: str) -> tuple:\n",
    "    query = [[query_string]]\n",
    "    \n",
    "    convert_to_lower([0], query)\n",
    "    to_be_removed = r'[.,?!/\\\\\\\"`\\-:()\\[\\]*|—’–]'\n",
    "    remove_chars([0], query, to_be_removed)\n",
    "    lemmatize([0], query, lemmatizer)\n",
    "    \n",
    "    return query[0][0], [word.strip() for word in re.split(r'\\s', query[0][0]) if word != '']\n",
    "\n",
    "def compute_tf_idf(query_string: str, data: list, field_id: int, index: dict) -> list:\n",
    "    query_lemmatized, query_lemmatized_list = parse_query(query_string)\n",
    "    scores = []\n",
    "    \n",
    "    for document in data:\n",
    "        document_words = [word.strip() for word in re.split(r'\\s', document[field_id]) if word != '']\n",
    "        common_words = set(query_lemmatized_list).intersection(set(document_words))\n",
    "        score = 0\n",
    "        \n",
    "        for word in common_words:\n",
    "            word_count_in_query = query_lemmatized.count(word)\n",
    "            word_count_in_doc = document[field_id].count(word)\n",
    "            \n",
    "            score += word_count_in_query * word_count_in_doc * math.log(len(data) + 1) / len(title_index[word])\n",
    "\n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. část - Použití a testování TF-IDF\n",
    "\n",
    "Nyní lze získat skóre pro titulky nebo text. Následujícím krokem je sjednocení výsledného skóre pro ohodnocení celého dokumentu. V případě dvou hodnot si vystačíme s parametrem $\\alpha$, který nám určuje jakou váhu má titulek a jakou samotný text dokumentu. <br>\n",
    "\n",
    "<center>\n",
    "$\n",
    "score(q,d) = \\alpha \\; TF\\_IDF\\_title(q,d) + (1-\\alpha) \\; TF\\_IDF\\_text(q,d)\n",
    "$\n",
    "</center>\n",
    "\n",
    "Při nastavení parametru $\\alpha$ na hodnotu 0.7 a vyhledávání dotazu \"coursera vs udacity machine learning\" by výsledky měly vypadat následovně:\n",
    "\n",
    "![output](sample_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     title                                        text                                         score\n",
      "276  coursera vs udacity for machine learning...  2018 be an exciting time for student of ...  63.716\n",
      " 67  every single machine learning course on ...  a year and a half ago I drop out of one ...  42.083\n",
      " 99  every single machine learning course on ...  a year and a half ago I drop out of one ...  42.083\n",
      "143  every single machine learning course on ...  a year and a half ago I drop out of one ...  42.083\n",
      " 19  every single machine learning course on ...  a year and a half ago I drop out of one ...  42.083\n",
      " 47  machine learning in a week   learn new s...  get into machine learning ml can seem li...  7.5647\n",
      "  9  what I learn from interview at multiple ...  over the past 8 month I ve be interview ...  7.2918\n",
      "160  what I learn from interview at multiple ...  over the past 8 month I ve be interview ...  7.2918\n",
      " 93  what I learn from interview at multiple ...  over the past 8 month I ve be interview ...  7.2918\n",
      "251  an augmentation base deep neural network...  overview in this post we will go over th...  5.4604\n",
      "227  be programmer head toward another bursti...  a friend of mine recently pose a questio...  3.9536\n",
      "261  be programmer head toward another bursti...  a friend of mine recently pose a questio...  3.9536\n",
      " 44  machine learning exercise in python part...  this content originally appear on curiou...  3.8647\n",
      "316  write an ai to win at pong from scratch ...  there s a huge difference between read a...  3.756\n",
      "154  explain simply how an ai program master ...  this be about alphago google deepmind go...  3.7342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.7\n",
    "q = 'coursera vs udacity machine learning'\n",
    "\n",
    "title_scores = np.array(compute_tf_idf(q, data, 4, title_index))\n",
    "content_scores = np.array(compute_tf_idf(q, data, 5, content_index))\n",
    "\n",
    "scores = alpha * title_scores + (1 - alpha) * content_scores\n",
    "order = np.argsort(-scores)\n",
    "\n",
    "print('{:3}  {:40}     {:40}     {:3.5}'.format('', 'title', 'text', 'scores'))\n",
    "\n",
    "for i in order[:15]:\n",
    "    print('{:3}  {:40}...  {:40}...  {:3.5}'.format(i, data[i][4][0:40], data[i][5][0:40], scores[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
