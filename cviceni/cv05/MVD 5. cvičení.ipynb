{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 5. cvičení\n",
    "\n",
    "## 1. část - TF-IDF s word embeddingy\n",
    "\n",
    "V minulém cvičení bylo za úkol implementovat TF-IDF algoritmus nad datasetem z Kagglu. Dnešní cvičení je rozšířením této úlohy s použitím word embeddingů. Lze použít předtrénované GloVe embeddingy ze 3. cvičení, nebo si v případě zájmu můžete vyzkoušet práci s Word2Vec od Googlu (najdete [zde](https://code.google.com/archive/p/word2vec/)).\n",
    "\n",
    "Cvičení by mělo obsahovat následující části:\n",
    "- Načtení článků a embeddingů\n",
    "- Výpočet document vektorů pomocí TF-IDF a word embeddingů \n",
    "    - Pro výpočet TF-IDF využijte [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) z knihovny sklearn\n",
    "    - Vážený průměr GloVe / Word2Vec vektorů\n",
    "\n",
    "<center>\n",
    "$\n",
    "doc\\_vector = \\frac{1}{|d|} \\sum\\limits_{w \\in d} TF\\_IDF(w) glove(w)\n",
    "$\n",
    "</center>\n",
    "\n",
    "- Dotaz bude transformován stejně jako dokument\n",
    "\n",
    "- Výpočet relevance pomocí kosinové podobnosti\n",
    "<center>\n",
    "$\n",
    "score(q,d) = cos\\_sim(query\\_vector, doc\\_vector)\n",
    "$\n",
    "</center>\n",
    "\n",
    "### Načtení článků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles(file_path: str) -> tuple:\n",
    "    with open(file_path, 'rt') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        data = [article for article in reader]\n",
    "        return data[0], data[1:]\n",
    "\n",
    "header, articles = load_articles('articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Načtení embeddingů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedded(file_path: str) -> tuple:\n",
    "    with open(file_path, 'rt') as file:\n",
    "        words = []\n",
    "        vectors = []\n",
    "\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' ')\n",
    "\n",
    "            words.append(parts[0])\n",
    "            vectors.append([float(part) for part in parts[1:]])\n",
    "    \n",
    "    word2idx = {word : index for index, word in enumerate(words)}\n",
    "    \n",
    "    return words, np.array(vectors), word2idx\n",
    "    \n",
    "w, v, w2i = load_embedded('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_field(field: str, to_be_removed: re.Pattern, lemmatizer) -> str:\n",
    "    field = field.lower()\n",
    "    field = re.sub(to_be_removed, '', field)\n",
    "    field = re.sub('\\s', ' ', field)\n",
    "    \n",
    "    return \" \".join([token.lemma_ for token in lemmatizer(field)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + Word2Vec a vytvoření doc vektorů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "to_be_removed = r'[.,?!/\\\\\\\"`\\-:()\\[\\]*|—’–]'\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # NLTK\n",
    "\n",
    "def adjust_articles(articles: list, fields: list) -> list:\n",
    "    adjusted_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        words = []\n",
    "\n",
    "        for num in fields:\n",
    "            modified_field = adjust_field(article[num], to_be_removed, lemmatizer)\n",
    "            words.extend([word.strip() for word in re.split(r'\\s', modified_field) if word != ''])\n",
    "            \n",
    "        adjusted_articles.append(words)\n",
    "        \n",
    "    return adjusted_articles\n",
    "\n",
    "def inverted_index(articles: list) -> dict:\n",
    "    index = {}\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        for word in article:\n",
    "            if word in index:\n",
    "                index[word].append(i)\n",
    "            else:\n",
    "                index[word] = [i]\n",
    "    \n",
    "    return index\n",
    "\n",
    "def filter_words(index: dict, glove: list) -> list:\n",
    "    blacklist = []\n",
    "    \n",
    "    index_words = set(index.keys())\n",
    "    glove_words = set(glove)\n",
    "    \n",
    "    blacklist = index_words - glove_words\n",
    "    \n",
    "    return blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_art = adjust_articles(articles, [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = inverted_index(adj_art)\n",
    "blacklist = filter_words(index, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.zeros(v.shape)\n",
    "\n",
    "for i, article in enumerate(adj_art):\n",
    "    adj_art[i] = list(set(article) - blacklist)\n",
    "\n",
    "for i, article in enumerate(adj_art):\n",
    "    vector = np.zeros([1, v.shape[1]])\n",
    "    \n",
    "    for word in article:\n",
    "            vector += (article.count(word) * math.log((len(adj_art) + 1) / len(index[word]))) * np.array(v[w2i[word]])\n",
    "\n",
    "    vectors[i, :] = (1 / len(article)) * vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformace dotazu a výpočet relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     title                                        text                                         score\n",
      "260  The $1700 great Deep Learning box: Assem...  Updated April 2018: Uses CUDA 9, cuDNN 7...  0.69648\n",
      "226  The $1700 great Deep Learning box: Assem...  Updated April 2018: Uses CUDA 9, cuDNN 7...  0.69648\n",
      "201  The $1700 great Deep Learning box: Assem...  Updated April 2018: Uses CUDA 9, cuDNN 7...  0.69648\n",
      "149  The $1700 great Deep Learning box: Assem...  Updated April 2018: Uses CUDA 9, cuDNN 7...  0.69648\n",
      "175  The $1700 great Deep Learning box: Assem...  Updated April 2018: Uses CUDA 9, cuDNN 7...  0.69648\n",
      "182  Into the Age of Context – Crossing the P...  I spent most of my early career proclaim...  0.68281\n",
      " 88  Reinforcement Learning from scratch – In...  Want to learn about applied Artificial I...  0.68206\n",
      "  4  Reinforcement Learning from scratch – In...  Want to learn about applied Artificial I...  0.68206\n",
      "157  Reinforcement Learning from scratch – In...  Want to learn about applied Artificial I...  0.68206\n",
      "214  How you can train an AI to convert your ...  Within three years, deep learning will c...  0.67782\n",
      "266  Deep Learning Achievements Over the Past...  At Statsbot, we’re constantly reviewing ...  0.67363\n",
      "103  Interview with Google’s Alfred Spector o...  Google’s a pretty good search engine, ri...  0.67114\n",
      " 22  Interview with Google’s Alfred Spector o...  Google’s a pretty good search engine, ri...  0.67114\n",
      "125  Why AI Research Loves Pac-Man – Tommy Th...  AI and Games is a crowdfunded YouTube se...  0.66969\n",
      "238  Why AI Research Loves Pac-Man – Tommy Th...  AI and Games is a crowdfunded YouTube se...  0.66969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27723/2257565246.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return (b @ a).reshape(1, b.shape[0]) / (np.linalg.norm(a) * np.linalg.norm(b, axis=1, keepdims=True)).reshape(1, b.shape[0])\n"
     ]
    }
   ],
   "source": [
    "def similarity_one_to_all(a, b):\n",
    "    return (b @ a).reshape(1, b.shape[0]) / (np.linalg.norm(a) * np.linalg.norm(b, axis=1, keepdims=True)).reshape(1, b.shape[0])\n",
    "\n",
    "q = 'coursera vs udacity machine learning'\n",
    "\n",
    "adjusted_query = adjust_articles([[q]], [0])\n",
    "index = inverted_index(adjusted_query)\n",
    "blacklist = filter_words(index, w)\n",
    "\n",
    "query_vect = np.zeros([1, v.shape[1]])\n",
    "\n",
    "for i, query in enumerate(adjusted_query):\n",
    "    adjusted_query[i] = list(set(query) - blacklist)\n",
    "\n",
    "for i, query in enumerate(adjusted_query):\n",
    "    vector = np.zeros([1, v.shape[1]])\n",
    "\n",
    "    for word in query:\n",
    "            vector += (query.count(word) * math.log((len(adjusted_query) + 1) / len(index[word]))) * np.array(v[w2i[word]])\n",
    "\n",
    "    query_vect[i, :] = (1 / len(query)) * vector\n",
    "\n",
    "scores = similarity_one_to_all(query_vect.flatten(), vectors).flatten()\n",
    "order = np.argsort(-scores)\n",
    "\n",
    "print('{:3}  {:40}     {:40}     {:3.5}'.format('', 'title', 'text', 'scores'))\n",
    "\n",
    "for i in order[:15]:\n",
    "    print('{:3}  {:40}...  {:40}...  {:3.5}'.format(i, articles[i][4][0:40], articles[i][5][0:40], scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Našeptávání\n",
    "\n",
    "Bonusem dnešního cvičení je našeptávání pomocí rekurentních neuronových sítí. Úkolem je vytvořit jednoduchou rekurentní neuronovou síť, která bude generovat text (character-level přístup). \n",
    "\n",
    "Optimální je začít po dokončení cvičení k předmětu ANS, kde se tato úloha řeší. \n",
    "\n",
    "Dataset pro učení vaší neuronové sítě naleznete na stránkách [Yahoo research](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&guccounter=1), lze využít např. i větší [Kaggle dataset](https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data) nebo vyhledat další dataset na [Google DatasetSearch](https://datasetsearch.research.google.com/).\n",
    "\n",
    "Vstupem bude rozepsaný dotaz a výstupem by měly být alespoň 3 dokončené dotazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
